# üìπ Captura de Audio - Flujo Completo

**Documentaci√≥n completa del sistema de captura de audio desde principio a fin**

---

## üìã √çndice

1. [Arquitectura General](#arquitectura-general)
2. [Programa Principal](#programa-principal)
3. [Fallback](#fallback)
4. [Renderizado](#renderizado)
5. [Integraci√≥n con Prompting](#integraci√≥n-con-prompting)
6. [Flujo Completo](#flujo-completo)

---

## üèóÔ∏è Arquitectura General

```
Usuario ‚Üí TranscriptArea (UI) ‚Üí useTranscript (Hook) ‚Üí MediaRecorder API
                                                      ‚Üì
                                              OpenAIWhisperService (Principal)
                                                      ‚Üì
                                              Whisper API (OpenAI)
                                                      ‚Üì
                                              Transcripci√≥n (texto)
                                                      ‚Üì
                                              PromptFactory-Canada
                                                      ‚Üì
                                              Vertex AI (An√°lisis)
```

---

## üéØ Programa Principal

### 1. Hook Principal: `useTranscript.ts`

**Ubicaci√≥n:** `src/hooks/useTranscript.ts`

**Responsabilidad:** Maneja todo el ciclo de vida de la grabaci√≥n de audio

**Caracter√≠sticas principales:**

#### Inicio de Grabaci√≥n (`startRecording`)

```typescript
// 1. Solicita acceso al micr√≥fono
const stream = await navigator.mediaDevices.getUserMedia({
  audio: {
    echoCancellation: true,
    noiseSuppression: true,
    autoGainControl: true,
    sampleRate: 48000
  }
});

// 2. Detecta mejor formato de audio para el dispositivo
let mimeType = 'audio/webm';
if (MediaRecorder.isTypeSupported('audio/webm;codecs=opus')) {
  mimeType = 'audio/webm;codecs=opus';
} else if (MediaRecorder.isTypeSupported('audio/mp4')) {
  mimeType = 'audio/mp4';
} // ... m√°s fallbacks

// 3. Crea MediaRecorder
const recorder = new MediaRecorder(stream, { mimeType });

// 4. Configura intervalos seg√∫n modo:
//    - LIVE: chunks cada 3 segundos (3000ms)
//    - DICTATION: chunks cada 10 segundos (10000ms)
const chunkInterval = mode === 'dictation' 
  ? DICTATION_CHUNK_INTERVAL_MS (10000ms)
  : LIVE_CHUNK_INTERVAL_MS (3000ms);

recorder.start(chunkInterval);
```

#### Procesamiento de Chunks (`ondataavailable`)

```typescript
recorder.ondataavailable = async (event: BlobEvent) => {
  if (event.data && event.data.size > 0) {
    // Normaliza tipo MIME (arregla errores comunes)
    let blobType = event.data.type
      .replace(/\/+/g, '/')      // Fix m√∫ltiples slashes
      .replace(/webrm/gi, 'webm') // Fix typo: webrm -> webm
    
    // Almacena chunk para transcripci√≥n final
    audioChunksRef.current.push(normalizedBlob);
    
    // ‚úÖ TRANSCRIBE SOLO EL PRIMER CHUNK para feedback en tiempo real
    // Los chunks subsecuentes pueden estar corruptos en formato webm
    // La transcripci√≥n completa se hace al detener la grabaci√≥n
    if (audioChunksRef.current.length === 1 && normalizedBlob.size >= 2000) {
      await transcribeChunk(normalizedBlob);
    }
  }
};
```

#### Transripci√≥n de Chunks (`transcribeChunk`)

```typescript
const transcribeChunk = async (chunkBlob: Blob) => {
  // Validaciones:
  // - Tama√±o m√≠nimo: 2KB
  // - Tipo MIME v√°lido (audio/*)
  // - Normalizaci√≥n de formato
  
  // Llama a OpenAIWhisperService
  const result = await OpenAIWhisperService.transcribe(normalizedBlob, {
    languageHint: languagePreference, // 'auto' | 'en' | 'es' | 'fr'
    mode // 'live' | 'dictation'
  });
  
  // Agrega texto al transcript
  appendTranscript(result.text);
  
  // Actualiza metadata (idioma detectado, confianza, duraci√≥n)
  setMeta({
    detectedLanguage: result.detectedLanguage,
    averageLogProb: result.averageLogProb,
    durationSeconds: result.durationSeconds
  });
};
```

#### Detenci√≥n de Grabaci√≥n (`onstop`)

```typescript
recorder.onstop = async () => {
  // 1. Crea blob final combinando todos los chunks
  const finalBlob = new Blob(audioChunksRef.current, { type: normalizedMimeType });
  
  // 2. Verifica tama√±o (l√≠mite: 25MB)
  const fileSizeMB = finalBlob.size / (1024 * 1024);
  
  if (fileSizeMB > 25) {
    // Maneja audio muy grande con timeout extendido
    await handleLargeAudio(finalBlob, normalizedMimeType);
  } else {
    // Transcribe audio completo
    await transcribeChunk(finalBlob);
  }
};
```

### 2. Servicio Principal: `OpenAIWhisperService.ts`

**Ubicaci√≥n:** `src/services/OpenAIWhisperService.ts`

**Responsabilidad:** Comunicaci√≥n con la API de Whisper de OpenAI

**Caracter√≠sticas:**

#### Configuraci√≥n

```typescript
- API_URL: "https://api.openai.com/v1/audio/transcriptions"
- MODEL: "gpt-4o-mini-transcribe" (o configurado via env)
- API_KEY: Desde variables de entorno
```

#### Prompt Cl√≠nico

```typescript
buildClinicalPrompt(mode: WhisperMode, promptOverride?: string): string {
  const baseLines = [
    "Clinical context: Canadian physiotherapy assessment in compliance with PHIPA/PIPEDA.",
    "Vocabulary bias: AiDuxCare, Niagara, physiotherapy, manual therapy, gait, cervical spine, lumbar spine...",
    "Respect Canadian English, Canadian French, and Latin American Spanish accents.",
    "Do not fabricate patient identifiers or personal health information."
  ];
  
  if (mode === "dictation") {
    baseLines.push("Mode: Post-session dictation with longer uninterrupted speech...");
  } else {
    baseLines.push("Mode: Live clinical conversation with back-and-forth dialogue...");
  }
  
  return baseLines.join("\n");
}
```

#### Mapeo de Formatos

```typescript
// Mapea tipos MIME a extensiones compatibles con Whisper
getWhisperCompatibleFilename(mimeType: string): string {
  const mimeToExt = {
    'audio/webm': 'webm',
    'audio/webm;codecs=opus': 'webm',
    'audio/mp4': 'mp4',
    'audio/mpeg': 'mp3',
    'audio/m4a': 'm4a',
    'audio/wav': 'wav',
    // ... m√°s formatos
  };
  
  return `clinical-audio.${extension}`;
}
```

#### Request a Whisper API

```typescript
static async transcribe(
  audioBlob: Blob,
  options?: WhisperTranscriptionOptions
): Promise<WhisperTranscriptionResult> {
  // 1. Valida API key
  this.ensureConfigured();
  
  // 2. Construye FormData
  const formData = this.buildFormData(audioBlob, options);
  
  // 3. Construye prompt cl√≠nico
  const prompt = this.buildClinicalPrompt(options?.mode, options?.promptOverride);
  formData.append("prompt", prompt);
  
  // 4. Env√≠a request a OpenAI
  const response = await fetch(this.API_URL, {
    method: 'POST',
    headers: {
      'Authorization': `Bearer ${this.API_KEY}`
    },
    body: formData
  });
  
  // 5. Procesa respuesta
  const data = await response.json();
  return this.mapResponse(data);
}
```

---

## ‚ö†Ô∏è Fallback

### Servicio de Fallback: `WebSpeechSTTService.ts`

**Ubicaci√≥n:** `src/services/WebSpeechSTTService.ts`

**Estado:** ‚ùå **DESHABILITADO**

**Raz√≥n:** 
- Causaba doble solicitud de permisos de micr√≥fono
- No se puede compartir el mismo stream con MediaRecorder
- Whisper es superior para transcripci√≥n m√©dica

**C√≥digo actual:**
```typescript
export class WebSpeechSTTService {
  // Todos los m√©todos son no-ops (no operaci√≥n)
  onSegment(_: (segment:any)=>void){/* noop */}
  onError(_: (err:any)=>void){/* noop */}
  async start(){/* noop */}
  async stop(){/* noop */}
}
```

**Nota:** El c√≥digo de Web Speech API est√° presente pero comentado/deshabilitado en `useTranscript.ts`:
- L√≠neas 121-124: `isWebSpeechAvailable()` - siempre retorna false
- L√≠neas 152-154: Comentario explicando por qu√© est√° deshabilitado

---

## üé® Renderizado

### Componente: `TranscriptArea.tsx`

**Ubicaci√≥n:** `src/components/workflow/TranscriptArea.tsx`

**Responsabilidad:** UI para captura y visualizaci√≥n de transcript

**Caracter√≠sticas:**

#### Estado del Bot√≥n de Grabaci√≥n

```typescript
// ACTUALMENTE DESHABILITADO (Coming Soon)
{isRecording ? (
  <button onClick={stopRecording}>
    <Square /> Stop Recording
  </button>
) : (
  <button onClick={startRecording} disabled={true}>
    <Play /> Start Recording (Coming Soon)
  </button>
)}
```

#### Banner Informativo

```typescript
<div className="border border-amber-200 bg-amber-50">
  <AlertCircle />
  <p>Voice recording is currently being improved.</p>
  <p>Please paste your transcript in the text area below.</p>
</div>
```

#### √Årea de Texto

```typescript
<textarea
  value={localTranscript}
  onChange={handleChange}
  placeholder="Paste or type clinical transcript here..."
  // Actualizaci√≥n en tiempo real con debounce (300ms)
/>
```

#### Visualizaci√≥n de Waveform

```typescript
{audioStream && (
  <AudioWaveform stream={audioStream} />
)}
```

**Nota:** El componente recibe todas las props de `useTranscript` pero el bot√≥n de grabaci√≥n est√° deshabilitado temporalmente.

### P√°gina Principal: `ProfessionalWorkflowPage.tsx`

**Ubicaci√≥n:** `src/pages/ProfessionalWorkflowPage.tsx`

**Responsabilidad:** Orquesta todo el flujo de trabajo

**Uso de useTranscript:**

```typescript
const {
  transcript,
  isRecording,
  isTranscribing,
  error: transcriptError,
  languagePreference,
  setLanguagePreference,
  mode,
  setMode,
  meta: transcriptMeta,
  audioStream,
  startRecording,
  stopRecording,
  setTranscript,
} = useTranscript();
```

**Pasa props a TranscriptArea:**

```typescript
<TranscriptArea
  recordingTime={recordingTime}
  isRecording={isRecording}
  startRecording={startRecording}
  stopRecording={stopRecording}
  transcript={transcript}
  setTranscript={setTranscript}
  transcriptError={transcriptError}
  transcriptMeta={transcriptMeta}
  languagePreference={languagePreference}
  setLanguagePreference={setLanguagePreference}
  mode={mode}
  setMode={setMode}
  isTranscribing={isTranscribing}
  isProcessing={isProcessing}
  audioStream={audioStream}
  handleAnalyzeWithVertex={handleAnalyzeWithVertex}
  // ... m√°s props
/>
```

---

## ü§ñ Integraci√≥n con Prompting

### 1. Handler de An√°lisis: `handleAnalyzeWithVertex`

**Ubicaci√≥n:** `src/pages/ProfessionalWorkflowPage.tsx` (l√≠nea 1619)

```typescript
const handleAnalyzeWithVertex = async () => {
  // 1. Asegura que transcript es string
  const transcriptText = typeof transcript === 'string' 
    ? transcript 
    : String(transcript || '');
  
  if (!transcriptText.trim()) return;
  
  try {
    // 2. Construye payload
    const payload = {
      text: transcriptText,                    // ‚Üê TRANSCRIPT AQU√ç
      lang: transcriptMeta?.detectedLanguage ?? languagePreference,
      mode,                                    // 'live' | 'dictation'
      timestamp: Date.now(),
      visitType: visitType === 'follow-up' ? 'follow-up' : 'initial'
    };
    
    // 3. Procesa con Niagara Processor
    await processText({
      ...payload,
      professionalProfile: professionalProfile || undefined
    });
  } catch (error) {
    // Manejo de errores
  }
};
```

### 2. Niagara Processor: `useNiagaraProcessor.ts`

**Ubicaci√≥n:** `src/hooks/useNiagaraProcessor.ts`

```typescript
const processText = async (params: {
  text: string;           // ‚Üê TRANSCRIPT
  lang?: string;
  mode?: 'live' | 'dictation';
  timestamp: number;
  visitType?: 'initial' | 'follow-up';
  professionalProfile?: ProfessionalProfile;
}) => {
  // 1. Construye prompt usando PromptFactory
  const prompt = PromptFactory.buildPrompt({
    transcript: params.text,              // ‚Üê TRANSCRIPT AQU√ç
    tests: selectedTests,
    attachments: session.attachments,
    professionalProfile: params.professionalProfile,
    visitType: params.visitType
  });
  
  // 2. Env√≠a a Vertex AI
  const response = await fetch(VERTEX_PROXY_URL, {
    method: 'POST',
    body: JSON.stringify({ prompt })
  });
  
  // 3. Procesa respuesta
  // ...
};
```

### 3. Prompt Factory: `PromptFactory-Canada.ts`

**Ubicaci√≥n:** `src/core/ai/PromptFactory-Canada.ts`

**Uso del transcript en el prompt:**

```typescript
export const buildCanadianPrompt = ({
  transcript,              // ‚Üê TRANSCRIPT AQU√ç
  contextoPaciente,
  instrucciones,
  visitType = 'initial',
  attachments,
  // ...
}: CanadianPromptParams): string => `
${PROMPT_HEADER}

[Patient Context]
${contextoPaciente}

## CLINICAL TRANSCRIPT

${transcript}              ‚Üê TRANSCRIPT INCLUIDO EN EL PROMPT

${attachments ? `
## CLINICAL ATTACHMENTS
${attachments.map(att => att.extractedText).join('\n\n')}
` : ''}

${instrucciones || (visitType === 'follow-up' 
  ? DEFAULT_INSTRUCTIONS_FOLLOWUP 
  : DEFAULT_INSTRUCTIONS_INITIAL)}
`;
```

**Nota:** El transcript se incluye directamente en el prompt que se env√≠a a Vertex AI para an√°lisis cl√≠nico.

---

## üîÑ Flujo Completo

### Paso a Paso:

1. **Usuario inicia grabaci√≥n:**
   - Click en "Start Recording" (actualmente deshabilitado)
   - `startRecording()` se llama desde `useTranscript`

2. **Acceso al micr√≥fono:**
   ```typescript
   navigator.mediaDevices.getUserMedia({ audio: {...} })
   ```
   - Solicita permisos al usuario
   - Retorna `MediaStream`

3. **MediaRecorder captura audio:**
   ```typescript
   const recorder = new MediaRecorder(stream, { mimeType });
   recorder.start(chunkInterval); // 3s (live) o 10s (dictation)
   ```

4. **Chunks de audio generados:**
   - Cada chunk es un `Blob` con audio
   - Se almacenan en `audioChunksRef.current`
   - Primer chunk se transcribe inmediatamente (feedback)

5. **Transcripci√≥n en tiempo real:**
   ```typescript
   OpenAIWhisperService.transcribe(chunkBlob, {
     languageHint: languagePreference,
     mode
   })
   ```
   - Env√≠a chunk a Whisper API
   - Recibe texto transcrito
   - Se agrega al transcript con `appendTranscript()`

6. **Usuario detiene grabaci√≥n:**
   - `stopRecording()` se llama
   - MediaRecorder se detiene
   - Stream se cierra

7. **Transcripci√≥n final:**
   ```typescript
   const finalBlob = new Blob(audioChunksRef.current);
   await transcribeChunk(finalBlob); // Transcripci√≥n completa
   ```

8. **Transcript listo:**
   - Texto completo disponible en `transcript` state
   - Metadata disponible (idioma, confianza, duraci√≥n)

9. **Usuario hace an√°lisis:**
   - Click en "Analyze with AI"
   - `handleAnalyzeWithVertex()` se llama
   - Transcript se pasa a `processText()`

10. **Construcci√≥n del prompt:**
    ```typescript
    PromptFactory.buildPrompt({
      transcript: transcriptText,  // ‚Üê TRANSCRIPT
      tests: selectedTests,
      attachments: session.attachments,
      visitType: 'initial' | 'follow-up'
    })
    ```

11. **Env√≠o a Vertex AI:**
    - Prompt completo (con transcript) se env√≠a a Vertex AI
    - Vertex AI analiza y genera respuesta cl√≠nica

12. **Resultados:**
    - An√°lisis cl√≠nico
    - Tests recomendados
    - Notas SOAP (si se solicita)

---

## üìä Modos de Operaci√≥n

### 1. Modo LIVE (`mode: 'live'`)

**Caracter√≠sticas:**
- Chunks cada **3 segundos** (3000ms)
- Prioriza latencia baja
- Feedback en tiempo real
- Ideal para conversaci√≥n cl√≠nica

**Prompt a Whisper:**
```
Mode: Live clinical conversation with back-and-forth dialogue; 
prioritise timely segmentation without losing context.
```

### 2. Modo DICTATION (`mode: 'dictation'`)

**Caracter√≠sticas:**
- Chunks cada **10 segundos** (10000ms)
- Prioriza completitud
- Tolerancia a silencios prolongados
- Ideal para dictado post-consulta

**Prompt a Whisper:**
```
Mode: Post-session dictation with longer uninterrupted speech; 
prioritise completeness over latency.
```

---

## üîß Configuraciones Clave

### Formatos de Audio Soportados

**Prioridad:**
1. `audio/webm;codecs=opus` (Chrome/Android)
2. `audio/webm` (fallback WebM)
3. `audio/mp4` (Safari/iOS)
4. `audio/mpeg` (fallback general)

### L√≠mites

- **Tama√±o m√≠nimo de chunk:** 2KB
- **Tama√±o m√°ximo de archivo:** 25MB
- **Timeout para audio grande:** 300 segundos (5 minutos)
- **Muestreo:** 48kHz

### Idiomas Soportados

- `auto` - Detecci√≥n autom√°tica
- `en` - English (EN-CA)
- `es` - Espa√±ol (LatAm)
- `fr` - Fran√ßais (Canada)

---

## üêõ Manejo de Errores

### Errores Comunes:

1. **Sin permisos de micr√≥fono:**
   ```typescript
   Error: El navegador no soporta captura de audio
   ```

2. **API key no configurada:**
   ```typescript
   Error: Servicio de transcripci√≥n no configurado
   ```

3. **Audio muy grande:**
   ```typescript
   Error: The audio file is very long (X MB) and transcription is taking too long
   ```

4. **Timeout:**
   ```typescript
   Error: Transcription timeout. Please check your connection
   ```

5. **Formato corrupto:**
   ```typescript
   Error: Audio format corrupted or unsupported
   ```

---

## üìù Notas Importantes

1. **Web Speech API est√° deshabilitado:**
   - No se usa como fallback
   - Causaba doble solicitud de permisos
   - Whisper es superior para transcripci√≥n m√©dica

2. **Bot√≥n de grabaci√≥n deshabilitado:**
   - Actualmente muestra "Coming Soon"
   - Usuario debe pegar transcript manualmente
   - Funcionalidad de grabaci√≥n existe pero est√° desactivada en UI

3. **Transcripci√≥n en dos fases:**
   - Primer chunk: transcripci√≥n inmediata (feedback)
   - Audio completo: transcripci√≥n final al detener

4. **Normalizaci√≥n de MIME types:**
   - Arregla errores comunes (webrm ‚Üí webm)
   - Normaliza m√∫ltiples slashes
   - Asegura compatibilidad con Whisper

5. **Prompt cl√≠nico incluido:**
   - Cada transcripci√≥n incluye contexto cl√≠nico
   - Vocabulario m√©dico sesgado
   - Compliance PHIPA/PIPEDA

---

## üìö Referencias de C√≥digo

- **Hook principal:** `src/hooks/useTranscript.ts`
- **Servicio principal:** `src/services/OpenAIWhisperService.ts`
- **Fallback (deshabilitado):** `src/services/WebSpeechSTTService.ts`
- **UI:** `src/components/workflow/TranscriptArea.tsx`
- **P√°gina:** `src/pages/ProfessionalWorkflowPage.tsx`
- **Prompting:** `src/core/ai/PromptFactory-Canada.ts`
- **Processor:** `src/hooks/useNiagaraProcessor.ts`

---

**√öltima actualizaci√≥n:** 2026-01-04  
**Estado:** Funcional pero grabaci√≥n deshabilitada en UI

