const { VertexAI } = require('@google-cloud/vertexai');
const ModelSelector = require('./ModelSelector');
const winston = require('winston');

const logger = winston.createLogger({
  level: 'info',
  format: winston.format.combine(
    winston.format.timestamp(),
    winston.format.json()
  ),
  transports: [new winston.transports.Console()]
});

class VertexAIClient {
  constructor() {
    this.projectId = process.env.GOOGLE_CLOUD_PROJECT_ID || 'aiduxcare-stt-20250706';
    this.location = process.env.VERTEX_AI_LOCATION || 'us-east1';
    
    // üîê CONFIGURACI√ìN DIN√ÅMICA DE MODELOS: Leer desde variables de entorno
    this.flashModel = process.env.MODEL_FLASH;
    this.proModel = process.env.MODEL_PRO;
    
    // Validaci√≥n cr√≠tica de configuraci√≥n
    if (!this.flashModel || !this.proModel) {
      const missingVars = [];
      if (!this.flashModel) missingVars.push('MODEL_FLASH');
      if (!this.proModel) missingVars.push('MODEL_PRO');
      
      throw new Error(`Missing critical AI model configuration in VertexAIClient: ${missingVars.join(', ')}. Please set environment variables.`);
    }
    
    // Usar modelo flash como default (estrategia 90/10)
    this.defaultModel = this.flashModel;
    
    // Inicializar ModelSelector para optimizaci√≥n inteligente
    this.modelSelector = new ModelSelector();
    
    // Inicializar cliente Vertex AI
    this.vertexAI = new VertexAI({
      project: this.projectId,
      location: this.location
    });
    
    logger.info('üöÄ VERTEXAI CLIENT INICIALIZADO CON CONFIGURACI√ìN DIN√ÅMICA', {
      proyecto: this.projectId,
      region: this.location,
      flashModel: this.flashModel,
      proModel: this.proModel,
      modeloPorDefecto: this.defaultModel
    });
  }

  /**
   * Procesa transcripci√≥n m√©dica con optimizaci√≥n basada en evidencia emp√≠rica
   * @param {string} transcription - Transcripci√≥n m√©dica
   * @param {string} prompt - Prompt generado
   * @param {Object} options - Opciones de procesamiento
   * @returns {Promise<Object>} Respuesta procesada con informaci√≥n de optimizaci√≥n
   */
  async processTranscription(transcription, prompt, options = {}) {
    logger.info('üß† INICIANDO PROCESAMIENTO CON OPTIMIZACI√ìN DE COSTOS');
    
    try {
      // Seleccionar modelo √≥ptimo basado en evidencia emp√≠rica
      let modelSelection;
      
      if (options.forceModel) {
        // Forzar modelo espec√≠fico (para testing)
        modelSelection = this.modelSelector.forceModel(options.forceModel);
      } else {
        // Selecci√≥n autom√°tica basada en evidencia
        modelSelection = this.modelSelector.selectOptimalModel(transcription, options);
      }
      
      logger.info('‚úÖ MODELO SELECCIONADO:', {
        modelo: modelSelection.selectedModel,
        razonamiento: modelSelection.reasoning,
        ahorro: modelSelection.costAnalysis?.savingsVsPro || 'N/A'
      });

      // Procesar con el modelo seleccionado
      const result = await this.processWithModel(
        transcription,
        prompt,
        modelSelection.selectedModel,
        options
      );

      // Enriquecer respuesta con informaci√≥n de optimizaci√≥n
      result.costOptimization = {
        modelUsed: modelSelection.selectedModel,
        redFlagsDetected: modelSelection.redFlagsDetected,
        reasoning: modelSelection.reasoning,
        costAnalysis: modelSelection.costAnalysis,
        empiricalBasis: modelSelection.empiricalBasis || 'Basado en evaluaci√≥n emp√≠rica',
        timestamp: modelSelection.timestamp
      };

      return result;

    } catch (error) {
      logger.error('‚ùå ERROR EN PROCESAMIENTO:', error);
      throw error;
    }
  }

  /**
   * Procesa transcripci√≥n con un modelo espec√≠fico
   * @param {string} transcription - Transcripci√≥n m√©dica
   * @param {string} prompt - Prompt generado
   * @param {string} modelName - Nombre del modelo a usar
   * @param {Object} options - Opciones de procesamiento
   * @returns {Promise<Object>} Respuesta del modelo
   */
  async processWithModel(transcription, prompt, modelName, options = {}) {
    try {
      // Configurar modelo seleccionado
      const modelConfig = this.getModelConfiguration(modelName);
      
      // Obtener instancia del modelo
      const model = this.vertexAI.getGenerativeModel({
        model: modelName,
        generationConfig: modelConfig.generationConfig,
        safetySettings: modelConfig.safetySettings
      });

      // Procesar con el modelo
      const startTime = Date.now();
      
      logger.info('üîÑ ENVIANDO PROMPT AL MODELO:', {
        modelo: modelName,
        longitudTranscripcion: transcription.length,
        longitudPrompt: prompt.length,
        promptPreview: prompt.substring(0, 200) + '...',
        promptType: typeof prompt
      });

      // Asegurar que el prompt sea string y est√© bien formateado
      const promptText = typeof prompt === 'string' ? prompt : JSON.stringify(prompt);
      
      const result = await model.generateContent(promptText);
      const processingTime = (Date.now() - startTime) / 1000;

      // üîç PASO 2: LOGGEAR RESPUESTA CRUDA DE VERTEX AI
      // CR√çTICO: Logging exhaustivo ANTES del parsing para debugging
      logger.info('üîç RESPUESTA CRUDA COMPLETA DE VERTEX AI:', {
        modelo: modelName,
        tiempoProcesamiento: `${processingTime}s`,
        resultCompleto: JSON.stringify(result, null, 2),
        responseCompleto: JSON.stringify(result.response, null, 2),
        candidatesLength: result.response?.candidates?.length || 0,
        hasResponse: !!result.response,
        hasCandidates: !!result.response?.candidates,
        timestamp: new Date().toISOString()
      });

      // Extraer respuesta con validaci√≥n exhaustiva
      const response = result.response;
      
      // üîç PASO 2: VALIDACI√ìN DETALLADA DE LA ESTRUCTURA DE RESPUESTA
      if (!response) {
        logger.error('‚ùå RESPUESTA DE VERTEX AI EST√Å VAC√çA:', {
          modelo: modelName,
          resultKeys: Object.keys(result || {}),
          resultType: typeof result
        });
        throw new Error('Vertex AI devolvi√≥ una respuesta vac√≠a');
      }

      if (!response.candidates || !Array.isArray(response.candidates)) {
        logger.error('‚ùå CANDIDATES NO ENCONTRADOS EN RESPUESTA DE VERTEX AI:', {
          modelo: modelName,
          responseKeys: Object.keys(response || {}),
          candidatesType: typeof response.candidates,
          candidatesValue: response.candidates
        });
        throw new Error('Vertex AI no devolvi√≥ candidates v√°lidos');
      }

      if (response.candidates.length === 0) {
        logger.error('‚ùå ARRAY DE CANDIDATES EST√Å VAC√çO:', {
          modelo: modelName,
          candidatesLength: response.candidates.length,
          responseCompleto: JSON.stringify(response, null, 2)
        });
        throw new Error('Vertex AI devolvi√≥ un array de candidates vac√≠o');
      }

      const candidate = response.candidates[0];
      
      if (!candidate.content || !candidate.content.parts || !Array.isArray(candidate.content.parts)) {
        logger.error('‚ùå ESTRUCTURA DE CONTENT INV√ÅLIDA EN CANDIDATE:', {
          modelo: modelName,
          candidateKeys: Object.keys(candidate || {}),
          contentKeys: Object.keys(candidate.content || {}),
          partsType: typeof candidate.content?.parts,
          candidateCompleto: JSON.stringify(candidate, null, 2)
        });
        throw new Error('Estructura de content inv√°lida en candidate de Vertex AI');
      }

      if (candidate.content.parts.length === 0) {
        logger.error('‚ùå ARRAY DE PARTS EST√Å VAC√çO:', {
          modelo: modelName,
          partsLength: candidate.content.parts.length,
          candidateCompleto: JSON.stringify(candidate, null, 2)
        });
        throw new Error('Vertex AI devolvi√≥ un array de parts vac√≠o');
      }

      const part = candidate.content.parts[0];
      
      if (!part.text) {
        logger.error('‚ùå TEXT NO ENCONTRADO EN PART:', {
          modelo: modelName,
          partKeys: Object.keys(part || {}),
          partCompleto: JSON.stringify(part, null, 2)
        });
        throw new Error('Vertex AI no devolvi√≥ texto v√°lido');
      }

      const text = part.text;

      // üîç PASO 2: LOGGING DETALLADO DEL TEXTO EXTRA√çDO
      logger.info('‚úÖ TEXTO EXTRA√çDO EXITOSAMENTE DE VERTEX AI:', {
        modelo: modelName,
        tiempoProcesamiento: `${processingTime}s`,
        longitudRespuesta: text.length,
        textoCompleto: text, // CR√çTICO: Todo el texto para debugging
        textoPreview: text.substring(0, 500) + (text.length > 500 ? '...' : ''),
        contieneBrackets: text.includes('{') && text.includes('}'),
        contieneJsonBlock: text.includes('```json'),
        primerosCaracteres: text.substring(0, 50),
        ultimosCaracteres: text.substring(Math.max(0, text.length - 50)),
        timestamp: new Date().toISOString()
      });

      return {
        text: text,
        modelUsed: modelName,
        processingTime: processingTime,
        metadata: {
          timestamp: new Date().toISOString(),
          projectId: this.projectId,
          location: this.location,
          inputLength: transcription.length,
          outputLength: text.length
        }
      };

    } catch (error) {
      logger.error('‚ùå ERROR PROCESANDO CON MODELO:', {
        modelo: modelName,
        error: error.message
      });

      // Analizar si se debe reintentar con modelo diferente
      if (this.shouldRetryWithDifferentModel(error)) {
        logger.info('üîÑ REINTENTANDO CON MODELO ALTERNATIVO...');
        
        const fallbackModel = this.getFallbackModel(modelName);
        if (fallbackModel !== modelName) {
          return await this.processWithModel(transcription, prompt, fallbackModel, options);
        }
      }

      throw error;
    }
  }

  /**
   * CONFIGURACI√ìN DIN√ÅMICA: Obtiene configuraci√≥n basada en variables de entorno
   * @param {string} modelName - Nombre del modelo (debe ser flashModel o proModel)
   * @returns {Object} Configuraci√≥n del modelo
   */
  getModelConfiguration(modelName) {
    // üîß CONFIGURACI√ìN DIN√ÅMICA: Mapear a configuraciones basadas en variables de entorno
    const configurations = {};
    
    // Configuraci√≥n para modelo Flash (optimizaci√≥n de costos)
    configurations[this.flashModel] = {
      generationConfig: {
        maxOutputTokens: 8192,
        temperature: 0.1,
        topP: 0.8,
        topK: 40
      },
      safetySettings: [
        {
          category: 'HARM_CATEGORY_HATE_SPEECH',
          threshold: 'BLOCK_ONLY_HIGH'
        },
        {
          category: 'HARM_CATEGORY_DANGEROUS_CONTENT',
          threshold: 'BLOCK_ONLY_HIGH'
        }
      ]
    };

    // Configuraci√≥n para modelo Pro (m√°xima calidad)
    configurations[this.proModel] = {
      generationConfig: {
        maxOutputTokens: 8192,
        temperature: 0.05,
        topP: 0.95,
        topK: 40,
        candidateCount: 1
      },
      safetySettings: [
        {
          category: 'HARM_CATEGORY_HATE_SPEECH',
          threshold: 'BLOCK_ONLY_HIGH'
        },
        {
          category: 'HARM_CATEGORY_DANGEROUS_CONTENT',
          threshold: 'BLOCK_ONLY_HIGH'
        }
      ]
    };

    // Retornar configuraci√≥n espec√≠fica o default a Flash
    return configurations[modelName] || configurations[this.flashModel];
  }

  /**
   * Determina si se debe reintentar con un modelo diferente
   * @param {Error} error - Error ocurrido
   * @returns {boolean} Si se debe reintentar
   */
  shouldRetryWithDifferentModel(error) {
    const retryableErrors = [
      'RESOURCE_EXHAUSTED',
      'QUOTA_EXCEEDED',
      'MODEL_NOT_AVAILABLE',
      'INVALID_ARGUMENT'
    ];

    return retryableErrors.some(errorType => 
      error.message.includes(errorType) || error.code === errorType
    );
  }

  /**
   * FALLBACK DIN√ÅMICO: Obtiene modelo de respaldo basado en configuraci√≥n de entorno
   * @param {string} currentModel - Modelo actual que fall√≥
   * @returns {string} Modelo de respaldo
   */
  getFallbackModel(currentModel) {
    // üîß CADENA DE FALLBACK DIN√ÅMICA basada en variables de entorno
    const fallbackChain = {};
    
    // Si falla Pro, usar Flash
    fallbackChain[this.proModel] = this.flashModel;
    
    // Si falla Flash, usar Pro como √∫ltimo recurso
    fallbackChain[this.flashModel] = this.proModel;
    
    // Retornar fallback espec√≠fico o default a Flash
    return fallbackChain[currentModel] || this.flashModel;
  }

  /**
   * INFORMACI√ìN DIN√ÅMICA: Obtiene informaci√≥n de modelos basada en configuraci√≥n de entorno
   * @returns {Object} Informaci√≥n de modelos configurados
   */
  getModelInfo() {
    return {
      projectId: this.projectId,
      location: this.location,
      availableModels: {
        flash: {
          name: this.flashModel,
          description: 'Modelo optimizado para costos y rendimiento',
          usageStrategy: 'Casos est√°ndar (90% de consultas)'
        },
        pro: {
          name: this.proModel,
          description: 'Modelo premium para casos cr√≠ticos',
          usageStrategy: 'Casos complejos con banderas rojas (10% de consultas)'
        }
      },
      defaultModel: this.defaultModel,
      configuration: 'DYNAMIC_FROM_ENVIRONMENT'
    };
  }

  /**
   * Obtiene estad√≠sticas de uso y costos
   * @returns {Object} Estad√≠sticas de optimizaci√≥n
   */
  getOptimizationStats() {
    return {
      modelsAvailable: Object.keys(this.modelSelector.getAvailableModels()),
      costOptimization: 'Habilitado',
      selectionStrategy: 'Basado en complejidad autom√°tica',
      maxSavings: 'Hasta 22.5x vs modelo premium'
    };
  }
}

module.exports = VertexAIClient; 