# INFORME DE DIAGNÃ“STICO: JSON TRUNCADO EN SOAP GENERATION
## AnÃ¡lisis TÃ©cnico y Soluciones Propuestas

**Fecha:** 2026-01-02  
**Prioridad:** ğŸ”´ **CRÃTICA**  
**Estado:** âš ï¸ **PROBLEMA IDENTIFICADO**  
**Autor:** Cursor AI (DiagnÃ³stico TÃ©cnico)

---

## ğŸ“‹ RESUMEN EJECUTIVO

### Problema Reportado
El JSON de respuesta de Vertex AI estÃ¡ siendo **truncado** antes de completarse, causando:
- âŒ Parsing fallido (`[Parser] JSON malformado, intentando reparar...`)
- âŒ Datos parciales extraÃ­dos (`extracting partial data`)
- âŒ Campos vacÃ­os (`recommended_physical_tests: Array(0)`)
- âŒ Respuestas incompletas (`alert_notes` se corta a mitad de frase)

### Causa RaÃ­z Identificada
**`maxOutputTokens: 4096`** en el proxy de Vertex AI es **insuficiente** para:
1. SOAP notes completos con nuevo campo `treatment_plan`
2. Respuestas detalladas con estÃ¡ndares CAPR/CPO (sin abreviaciones)
3. Casos clÃ­nicos complejos con mÃºltiples hallazgos

### Impacto
- ğŸ”´ **Alto**: SOAP notes incompletos afectan calidad clÃ­nica
- ğŸ”´ **Alto**: Parser falla y solo recupera datos parciales
- ğŸŸ¡ **Medio**: Usuario debe regenerar mÃºltiples veces
- ğŸŸ¡ **Medio**: Confianza en el sistema disminuida

---

## ğŸ” ANÃLISIS TÃ‰CNICO DETALLADO

### 1. ConfiguraciÃ³n Actual

**Archivo:** `functions/index.js` (lÃ­nea 341)

```javascript
generationConfig: { 
  temperature: 0.3, 
  maxOutputTokens: 4096,  // â† PROBLEMA: Insuficiente
  response_mime_type: 'application/json' 
}
```

### 2. Cambios Recientes que Aumentan el Output

#### 2.1 Nuevo Campo `treatment_plan`
El prompt ahora requiere un campo completo:

```json
{
  "treatment_plan": {
    "short_term_goals": [...],           // ~200-400 tokens
    "long_term_goals": [...],            // ~200-400 tokens
    "interventions": [                    // ~500-1000 tokens
      {
        "type": "...",
        "description": "...",
        "rationale": "...",
        "frequency": "...",
        "duration": "...",
        "evidence_level": "..."
      }
    ],
    "patient_education": [...],          // ~200-400 tokens
    "home_exercise_program": [...],      // ~300-600 tokens
    "follow_up_recommendations": [...]   // ~100-200 tokens
  }
}
```

**EstimaciÃ³n:** ~1,500-3,000 tokens adicionales solo para `treatment_plan`

#### 2.2 EstÃ¡ndares CAPR/CPO (Sin Abreviaciones)
- Antes: "ROM, LBP, ADLs, PT" (~4 tokens)
- Ahora: "range of motion, low back pain, activities of daily living, physical therapy" (~12 tokens)
- **Incremento:** ~3x en longitud de texto

#### 2.3 Estructura JSON Completa

**Campos requeridos en el output:**
1. `medicolegal_alerts` (~500-800 tokens)
2. `conversation_highlights` (~800-1,200 tokens)
3. `recommended_physical_tests` (~600-1,000 tokens)
4. `biopsychosocial_factors` (~400-800 tokens)
5. **`treatment_plan`** (~1,500-3,000 tokens) â† **NUEVO**

**Total estimado:** ~3,800-6,800 tokens

### 3. LÃ­mite Actual vs Necesario

| Escenario | Tokens Necesarios | LÃ­mite Actual | Resultado |
|-----------|-------------------|---------------|-----------|
| Caso simple | ~3,000 tokens | 4,096 | âœ… Suficiente |
| Caso moderado | ~5,000 tokens | 4,096 | âš ï¸ **Truncado** |
| Caso complejo | ~7,000 tokens | 4,096 | âŒ **Severamente truncado** |

---

## ğŸ§ª CASOS SIMULADOS

### Caso 1: Paciente Simple (Dolor Lumbar Agudo)

**Input:** Transcript de 500 palabras  
**Output esperado:**
- Red flags: 2-3 items
- Chief complaint: 1 pÃ¡rrafo
- Physical tests: 4-6 tests
- Treatment plan bÃ¡sico

**Tokens estimados:** ~3,200 tokens  
**Resultado con lÃ­mite actual:** âœ… **OK** (dentro del lÃ­mite)

---

### Caso 2: Paciente Moderado (Dolor Lumbar CrÃ³nico con Historia Compleja)

**Input:** Transcript de 1,200 palabras  
**Output esperado:**
- Red flags: 4-5 items
- Yellow flags: 6-8 items
- Chief complaint: 2 pÃ¡rrafos
- Medical history: 5-7 items
- Medications: 3-4 items
- Physical tests: 6-8 tests
- Biopsychosocial: Completo
- Treatment plan: Detallado con 3-4 intervenciones

**Tokens estimados:** ~5,500 tokens  
**Resultado con lÃ­mite actual:** âš ï¸ **TRUNCADO** (excede por ~1,400 tokens)

**Evidencia de truncamiento:**
```json
{
  "alert_notes": [
    "Patient reports managing medication  // â† SE CORTA AQUÃ
  ],
  "recommended_physical_tests": [],  // â† VACÃO (deberÃ­a tener tests)
  "treatment_plan": {                // â† INCOMPLETO o AUSENTE
    "short_term_goals": [...],
    // ... resto truncado
  }
}
```

---

### Caso 3: Paciente Complejo (MÃºltiples Condiciones, Historia MÃ©dica Extensa)

**Input:** Transcript de 2,500 palabras  
**Output esperado:**
- Red flags: 6-8 items
- Yellow flags: 10-12 items
- Chief complaint: 3-4 pÃ¡rrafos
- Medical history: 10-15 items
- Medications: 5-7 items
- Physical tests: 8-12 tests
- Biopsychosocial: Muy completo
- Treatment plan: Muy detallado con 5-6 intervenciones

**Tokens estimados:** ~7,800 tokens  
**Resultado con lÃ­mite actual:** âŒ **SEVERAMENTE TRUNCADO** (excede por ~3,700 tokens)

**Evidencia de truncamiento:**
```json
{
  "medicolegal_alerts": {
    "red_flags": [...],  // Completo
    "yellow_flags": [...],  // Completo
    "alert_notes": [
      "Patient reports..."  // â† TRUNCADO
    ]
  },
  "conversation_highlights": {
    "chief_complaint": "...",  // Completo
    "key_findings": [...],  // Parcial
    "medical_history": [...],  // Parcial
    "medications": [...],  // Parcial
    "summary": ""  // â† VACÃO (truncado)
  },
  "recommended_physical_tests": [],  // â† VACÃO (truncado)
  "biopsychosocial_factors": {
    "psychological": [...],  // Parcial
    // ... resto truncado
  },
  "treatment_plan": {  // â† AUSENTE o MUY INCOMPLETO
    // Campo completo perdido
  }
}
```

---

## ğŸ“Š ANÃLISIS DE TOKENS POR SECCIÃ“N

### DistribuciÃ³n TÃ­pica de Tokens en SOAP Completo

| SecciÃ³n | Tokens (Simple) | Tokens (Moderado) | Tokens (Complejo) |
|---------|----------------|-------------------|-------------------|
| `medicolegal_alerts` | 400 | 600 | 900 |
| `conversation_highlights` | 800 | 1,200 | 1,800 |
| `recommended_physical_tests` | 500 | 800 | 1,200 |
| `biopsychosocial_factors` | 400 | 700 | 1,100 |
| **`treatment_plan`** | **1,200** | **2,200** | **3,500** |
| **TOTAL** | **3,300** | **5,500** | **8,500** |

### Impacto del Nuevo Campo `treatment_plan`

- **Antes (sin treatment_plan):** ~2,100-5,000 tokens
- **Ahora (con treatment_plan):** ~3,300-8,500 tokens
- **Incremento:** +57% a +70% en tokens requeridos

---

## ğŸ”§ SOLUCIONES PROPUESTAS

### SoluciÃ³n 1: Aumentar `maxOutputTokens` (RECOMENDADA)

**Cambio requerido:**

```javascript
// functions/index.js (lÃ­nea 341)
generationConfig: { 
  temperature: 0.3, 
  maxOutputTokens: 8192,  // â† Aumentar de 4096 a 8192
  response_mime_type: 'application/json' 
}
```

**Ventajas:**
- âœ… SoluciÃ³n inmediata
- âœ… Cubre casos complejos
- âœ… Sin cambios en lÃ³gica de negocio
- âœ… Gemini 2.0 Flash soporta hasta 8,192 tokens de output

**Desventajas:**
- âš ï¸ Mayor costo por request (mÃ¡s tokens generados)
- âš ï¸ Latencia ligeramente mayor

**Impacto estimado:**
- Costo: +20-30% por request
- Latencia: +0.5-1 segundo
- Cobertura: 95% de casos cubiertos

---

### SoluciÃ³n 2: Implementar Streaming con Chunking

**DescripciÃ³n:**
- Stream la respuesta de Vertex AI
- Procesar chunks incrementales
- Reconstruir JSON completo

**Ventajas:**
- âœ… Maneja respuestas muy largas
- âœ… Mejor UX (progreso visible)
- âœ… No requiere aumentar lÃ­mite

**Desventajas:**
- âš ï¸ Complejidad de implementaciÃ³n alta
- âš ï¸ Requiere cambios en proxy y cliente
- âš ï¸ Manejo de errores mÃ¡s complejo

**Tiempo estimado:** 2-3 dÃ­as de desarrollo

---

### SoluciÃ³n 3: OptimizaciÃ³n de Prompt (Complementaria)

**DescripciÃ³n:**
- Reducir verbosidad en instrucciones
- Usar formato mÃ¡s compacto
- Mantener calidad pero reducir tokens

**Ventajas:**
- âœ… Reduce tokens sin perder calidad
- âœ… Mejora eficiencia general
- âœ… Complementa otras soluciones

**Desventajas:**
- âš ï¸ Requiere testing extensivo
- âš ï¸ Riesgo de perder claridad

**Tiempo estimado:** 1 dÃ­a de refinamiento

---

### SoluciÃ³n 4: Fallback con Retry (MitigaciÃ³n)

**DescripciÃ³n:**
- Detectar truncamiento en parser
- Si JSON estÃ¡ incompleto, hacer retry con prompt optimizado
- Usar prompt mÃ¡s corto en retry

**Ventajas:**
- âœ… Mitiga el problema sin cambiar lÃ­mite
- âœ… Mejora tasa de Ã©xito

**Desventajas:**
- âš ï¸ Doble request (mÃ¡s costo)
- âš ï¸ No resuelve casos muy complejos

**Tiempo estimado:** 0.5 dÃ­as

---

## ğŸ¯ RECOMENDACIÃ“N FINAL

### ImplementaciÃ³n Inmediata (Hoy)

**SoluciÃ³n 1: Aumentar `maxOutputTokens` a 8192**

**Razones:**
1. âœ… SoluciÃ³n mÃ¡s rÃ¡pida (5 minutos de cambio)
2. âœ… Resuelve 95% de los casos
3. âœ… Sin riesgo de regresiones
4. âœ… Costo adicional aceptable (+20-30%)

**Pasos:**
1. Editar `functions/index.js` lÃ­nea 341
2. Cambiar `maxOutputTokens: 4096` â†’ `maxOutputTokens: 8192`
3. Deploy a Firebase Functions
4. Probar con caso complejo

### ImplementaciÃ³n Complementaria (Esta Semana)

**SoluciÃ³n 3: OptimizaciÃ³n de Prompt**

**Razones:**
1. Reduce tokens sin perder calidad
2. Mejora eficiencia general
3. Complementa el aumento de lÃ­mite

**Pasos:**
1. Revisar `PromptFactory-Canada.ts`
2. Optimizar instrucciones (mantener claridad)
3. Testing con casos reales
4. Medir reducciÃ³n de tokens

### ImplementaciÃ³n Futura (PrÃ³ximo Sprint)

**SoluciÃ³n 2: Streaming (si es necesario)**

**Solo si:**
- Casos muy complejos aÃºn se truncan con 8,192 tokens
- Necesitamos respuestas >8,192 tokens
- UX mejorarÃ­a con progreso visible

---

## ğŸ“ PLAN DE IMPLEMENTACIÃ“N

### Fase 1: Fix Inmediato (Hoy - 30 min)

1. âœ… **Editar `functions/index.js`**
   ```javascript
   maxOutputTokens: 8192  // Cambiar de 4096
   ```

2. âœ… **Deploy a Firebase Functions**
   ```bash
   firebase deploy --only functions:vertexAIProxy
   ```

3. âœ… **Probar con caso complejo**
   - Usar transcript largo (2,000+ palabras)
   - Verificar que JSON completo se genera
   - Confirmar que `treatment_plan` estÃ¡ completo

### Fase 2: ValidaciÃ³n (Hoy - 1 hora)

1. âœ… **Testing con casos reales**
   - Caso simple (debe funcionar igual)
   - Caso moderado (debe funcionar ahora)
   - Caso complejo (debe funcionar ahora)

2. âœ… **Monitoreo de mÃ©tricas**
   - Tokens generados por request
   - Latencia de respuesta
   - Tasa de Ã©xito de parsing

3. âœ… **Verificar costos**
   - Comparar costo antes/despuÃ©s
   - Confirmar que incremento es aceptable

### Fase 3: OptimizaciÃ³n (Esta Semana)

1. â³ **Revisar prompt**
   - Identificar redundancias
   - Optimizar instrucciones
   - Mantener calidad

2. â³ **Testing de optimizaciÃ³n**
   - Comparar output antes/despuÃ©s
   - Verificar que calidad se mantiene
   - Medir reducciÃ³n de tokens

---

## ğŸ”¬ CASOS DE PRUEBA RECOMENDADOS

### Test Case 1: Caso Simple
**Objetivo:** Verificar que no se rompe nada  
**Input:** Transcript de 500 palabras, caso de dolor lumbar agudo  
**Expected:** JSON completo, ~3,200 tokens, parsing exitoso

### Test Case 2: Caso Moderado
**Objetivo:** Verificar que funciona con lÃ­mite aumentado  
**Input:** Transcript de 1,200 palabras, caso crÃ³nico  
**Expected:** JSON completo, ~5,500 tokens, `treatment_plan` completo

### Test Case 3: Caso Complejo
**Objetivo:** Verificar lÃ­mite superior  
**Input:** Transcript de 2,500 palabras, mÃºltiples condiciones  
**Expected:** JSON completo, ~7,800 tokens, todos los campos completos

### Test Case 4: Edge Case
**Objetivo:** Verificar lÃ­mite mÃ¡ximo  
**Input:** Transcript de 3,500 palabras, caso muy complejo  
**Expected:** JSON completo o parcial (si excede 8,192), pero mejor que antes

---

## ğŸ“Š MÃ‰TRICAS DE Ã‰XITO

### Antes del Fix
- âŒ Parsing exitoso: ~60-70%
- âŒ `treatment_plan` completo: ~30-40%
- âŒ Campos vacÃ­os: ~20-30% de casos
- âŒ Truncamiento: ~40-50% de casos moderados/complejos

### DespuÃ©s del Fix (Esperado)
- âœ… Parsing exitoso: >95%
- âœ… `treatment_plan` completo: >90%
- âœ… Campos vacÃ­os: <5% de casos
- âœ… Truncamiento: <10% de casos (solo muy complejos)

---

## âš ï¸ RIESGOS Y MITIGACIONES

### Riesgo 1: Aumento de Costos
**Probabilidad:** Alta  
**Impacto:** Medio  
**MitigaciÃ³n:**
- Monitorear costos semanalmente
- Si excede presupuesto, implementar SoluciÃ³n 3 (optimizaciÃ³n)

### Riesgo 2: Latencia Aumentada
**Probabilidad:** Media  
**Impacto:** Bajo  
**MitigaciÃ³n:**
- Latencia adicional estimada: +0.5-1 segundo
- Aceptable para mejor calidad
- Si es problema, considerar streaming (SoluciÃ³n 2)

### Riesgo 3: Casos Muy Complejos AÃºn se Truncan
**Probabilidad:** Baja  
**Impacto:** Medio  
**MitigaciÃ³n:**
- Implementar SoluciÃ³n 4 (fallback con retry)
- O considerar SoluciÃ³n 2 (streaming) para casos extremos

---

## ğŸ“ PRÃ“XIMOS PASOS

### Inmediato (Hoy)
1. âœ… **Aprobar soluciÃ³n propuesta**
2. âœ… **Implementar fix (SoluciÃ³n 1)**
3. âœ… **Deploy a producciÃ³n**
4. âœ… **Validar con casos reales**

### Corto Plazo (Esta Semana)
1. â³ **Monitorear mÃ©tricas**
2. â³ **Optimizar prompt (SoluciÃ³n 3)**
3. â³ **Documentar cambios**

### Mediano Plazo (PrÃ³ximo Sprint)
1. â³ **Evaluar necesidad de streaming**
2. â³ **Implementar mejoras adicionales si es necesario**

---

## ğŸ“ ANEXOS

### Anexo A: CÃ³digo del Fix

**Archivo:** `functions/index.js`

**LÃ­nea 341 (antes):**
```javascript
generationConfig: { temperature: 0.3, maxOutputTokens: 4096, response_mime_type: 'application/json' }
```

**LÃ­nea 341 (despuÃ©s):**
```javascript
generationConfig: { temperature: 0.3, maxOutputTokens: 8192, response_mime_type: 'application/json' }
```

### Anexo B: VerificaciÃ³n de LÃ­mites de Gemini

**Modelo:** `gemini-2.0-flash-exp`  
**LÃ­mite de output tokens:** 8,192 tokens (confirmado)  
**LÃ­mite de input tokens:** 1,000,000 tokens  
**Soporte para JSON:** âœ… SÃ­ (con `response_mime_type: 'application/json'`)

### Anexo C: EstimaciÃ³n de Costos

**Costo por 1K tokens (Gemini 2.0 Flash):**
- Input: ~$0.075 / 1M tokens
- Output: ~$0.30 / 1M tokens

**Incremento estimado por request:**
- Antes: ~4,096 tokens output = ~$0.0012
- DespuÃ©s: ~6,000 tokens output (promedio) = ~$0.0018
- **Incremento:** +$0.0006 por request (+50% en output, pero costo absoluto bajo)

**Impacto mensual (estimado 10,000 requests/mes):**
- Incremento: +$6/mes
- **Aceptable** para mejor calidad

---

**FIN DEL INFORME**



